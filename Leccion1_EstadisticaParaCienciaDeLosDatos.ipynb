{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQr5J5eomeak"
   },
   "source": [
    "# Estadística para Ciencia de los Datos - Lección 1\n",
    "\n",
    "Autor: Saúl Calderón, Juan Esquivel, Jorge Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHZ00bTEHD2N"
   },
   "source": [
    "# Probabilidad\n",
    "\n",
    "La teoría de la probabilidad es de suma importancia para muchos de los\n",
    "algoritmos en el aprendizaje automático y el procesamiento de señales, pues\n",
    "es habitual el lidiar con incertidumbre. La teoría de la probabilidad provee un marco de trabajo consistente para la cuantificación y manipulación de la incertidumbre. A continuación veremos un repaso de conceptos\n",
    "básicos de probabilidades. Existen dos ramas o enfoques, la teoría de probabilidades **frecuentista**, basada en la frecuencia de ocurrencias de eventos, y la\n",
    "**bayesiana**, que define las probabilidades de acuerdo a suposiciones previas sobre el fenómeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYbwAPtEK7dF"
   },
   "source": [
    "## Axiomas de la probabilidad\n",
    "\n",
    "\n",
    "\n",
    "*   **Conjunto de puntos muestrales (o espacio muestral) $\\Omega$**: Conjunto de todos los resultados posibles de\n",
    "un experimento. El resultado de un experimento puede conceptualizarse\n",
    "como una descripción completa de un estado del mundo real, al finalizar\n",
    "un experimento, y se denota como $\\omega \\in \\Omega$\n",
    "*   **Conjunto de eventos (o espacio de eventos) $\\mathcal{F}$**: A un conjunto $A$ de posibles salidas $\\omega$ de un experimento se le llama un **evento**, por lo que entonces $A$ ⊆ Ω. $\\mathcal{F}$ es un espacio de eventos al que\n",
    "pertenecen uno o más eventos $A_i$, por lo que entonces $A_i \\in \\mathcal{F}$. El conjunto $\\mathcal{F}$ satisface las siguientes propiedades:\n",
    "\n",
    "  * El conjunto vacío siempre pertenece a $\\mathcal{F}:\\emptyset \\in \\mathcal{F}$\n",
    "  * $A_1, A_2, ..., A_n \\in \\mathcal{F} \\implies \\bigcup\\limits_{i=0}^{n} A_i \\in \\mathcal{F}$\n",
    "  \n",
    "* Propiedades básicas de la función de probabilidad:\n",
    "\n",
    "  * $p(\\emptyset) = 0$\n",
    "  * $p(\\Omega) =1$\n",
    "  * $0 \\leq p(A) \\leq 1, \\forall A \\in \\mathcal{F}$\n",
    "  * Si $ A_1, A_2, ..., A_n $ son eventos disjuntos tal que $A_i \\cap A_j = \\emptyset$, si $ i \\neq j $ entonces se tiene que: $p(\\bigcup\\limits_{i=0}^{n} A_i) = \\sum\\limits_{i=0}^{n} p(A_i) $\n",
    "  \n",
    "### Ejemplo 1\n",
    "Defina el evento de tirar un dado de seis caras. El conjunto de puntos muestrales en este caso viene dado por $\\Omega=\\left\\{ 1,2,3,4,5,6\\right\\} $ con $\\omega_{1}=1,\\omega_{2}=2,\\ldots$ etc. El espacio de eventos más simple es $\\mathcal{F}=\\left\\{ \\emptyset,\\Omega\\right\\} $, para el cual se define  $p\\left(\\emptyset\\right)=0$ y $p\\left(\\Omega\\right)=1$. Otro espacio de eventos posible  $\\mathcal{F}$ es el conjunto de todos los subconjuntos de $\\Omega$. Para este último espacio de eventos, se puede asignar la probabilidad de cada conjunto $A_{i}$ en tal espacio de eventos $\\mathcal{F}$ como $\\frac{k}{n}$donde $k$ es la cantidad de elementos $\\left|A_{i}\\right|$ o cardinalidad y $n=\\left|\\Omega\\right|=6$.  Por ejemplo $p\\left(A_{1}=\\left\\{ 1,2,3,4\\right\\} \\right)=\\frac{4}{6}$\n",
    "\n",
    "### Ejemplo 2\n",
    "Imagine dos cajas, una roja y otra azul. En la caja roja existen 2 manzanas verdes y 6 naranjas, mientras que en la azul existen 3 manzanas verdes y una naranja, como se ilustra en la siguiente figura. Se definen entonces dos conjuntos de puntos muestrales para dos espacios de eventos distintos: $\\Omega_{1}=\\left\\{ r,a\\right\\} $ el cual se refiere a la escogencia de la caja azul o roja y\n",
    "$\\Omega_{2}=\\left\\{ n,v\\right\\} $ que contiene los resultados experimentales de escoger una naranja o una manzana verde. El espacio de eventos correspondiente a la escogencia de las cajas se define como $\\mathcal{F}_{1}=\\left\\{A_{1}=\\left\\{ r\\right\\} ,A_{2}=\\left\\{ a\\right\\} \\right\\} $, con probabilidades $p\\left(A_{1}\\right)=0.4$ y $p\\left(A_{2}\\right)=0.6$. Observe que dado que\n",
    "$A_{1}\\cap A_{2}=\\emptyset$, entonces $p\\left(\\bigcup\\limits_{i=1}^{2} A_i\\right)=A_{1}+A_{2}=0.4 + 0.6=1$. Más adelante definiremos el espacio de eventos correspondiente a la escogencia de las frutas, pues observe que esto depende de la caja escogida, asociado con una **probabilidad condicional**\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1isFSFkup_XGfXd2cTUsFOwTr_09BbiDa)\n",
    "\n",
    "### Propiedades de los eventos\n",
    "* Si $A\\subseteq B\\Rightarrow p\\left(A\\right)\\leq p\\left(B\\right)$\n",
    "* Cota de la intersección $p\\left(A\\cap B\\right)\\leq\\min\\left(p\\left(A\\right),p\\left(B\\right)\\right)$\n",
    "* Cota de la unión $p\\left(A\\cup B\\right)\\leq p(A)+p(B)$\n",
    "* Complemento $p\\left(\\Omega-A\\right)=1-p(A)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdg6UigMX91t"
   },
   "source": [
    "## Variables aleatorias\n",
    "Considere el experimento de tirar una moneda 4 veces, con el objetivo de saber el número de veces que sale corona. En este caso el conjunto de puntos muestrales podría definirse a partir de todas las secuencias posibles de escudos o coronas que salen al tirar la moneda 4 veces $\\Omega=\\left\\{aaaa, aaab, aaba, ...\\right\\} $, donde $a$ implica que salió corona y $b$ implica que salió escudo. Sin embargo, de acuerdo a nuestro objetivo, no es necesario saber la probabilidad de obtener una secuencia particular de escudos o coronas. En cambio, es más útil expresar lo anterior en términos de una función real que denote por ejemplo la cantidad de veces que aparece corona  después de 4 lanzamientos de la moneda. Tales funciones son conocidas como **variables aleatorias**. Más formalmente, una variable aleatoria $X$ o $X\\left(\\omega\\right)$ es una función que asigna un número real a cada posible resultado $w$ de un experimento aleatorio, $X:\\Omega\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "\n",
    "Una **variable aleatoria discreta** es aquella que solo puede tomar un número contable de valores. El ejemplo de las 4 tiradas de la moneda es un caso en el que se define una variable aleatoria discreta. Formalmente,\n",
    "la probabilidad de que una variable discreta tome un valor $x_i$ viene dado por $ p\\left(X=x_i\\right)$\n",
    "\n",
    "Una **variable aleatoria continua** toma una infinita cantidad de valores posibles, por lo que usualmente se define la probabilidad de que la variable aleatoria tome un valor en el intervalo $[a,b]$ $/$ $a\\in\\mathbb{R} \\land b\\in\\mathbb{R}$ como $p\\left(a\\leq X\\leq b\\right)$.\n",
    "\n",
    "\n",
    " ### Ejemplos\n",
    "\n",
    " Algunos ejemplos de variables aleatorias continuas son:\n",
    " * Tiempo\n",
    " * Corriente eléctrica\n",
    " * Temperatura\n",
    "\n",
    "Algunos ejemplos de variables aleatorias discretas son:\n",
    " * El resultado de lanzar un dado\n",
    " * El número de estudiantes presentes\n",
    " * La edad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMFpJG8UYvY_"
   },
   "source": "## Probabilidad conjunta y condicional\n\nConsidere dos variables aleatorias discretas $X$ y $Y$, con sus respectivos codominios $\\Omega_{X}=\\left\\{ x_{1},x_{2},\\ldots x_{M}\\right\\} $ y $\\Omega_{Y}=\\left\\{ y_{1},y_{2},\\ldots y_{L}\\right\\} $. Se toman un total de $N$ puntos muestrales de ambas variables aleatorias donde el número de experimentos en los que la variable aleatoria $X=x_{i}$ y la otra variable aleatoria es $Y=y_{j}$ está definido por $n_{i,j}$, de modo que: $N=\\sum_{i}^{M}\\sum_{j}^{L}n_{i,j}.$\n\nAdemás, se define el número de experimentos en las que la variable aleatoria $X=x_{i}$ como $c_{i}=\\sum_{j}^{L}n_{i,j},\n$ y de igual forma, el número de experimentos en los que la variable aleatoria $Y=y_{j}$ como $r_{j}=\\sum_{i}^{M}n_{i,j}.$\n\n\nLa figura ilustra el ejemplo en el que $M=5$ y $L=3$.\n\n\n![](https://drive.google.com/uc?export=view&id=1Rv4RtlSUxKQnxEG8CvVCVdoFRIrx7-2o)\n\n### Probabilidad conjunta:\nDesde un punto de vista probabilístico frecuentista en el que se aproxima\n una función de densidad de probabilidad, a medida que $N\\rightarrow\\infty$, se define la probabilidad conjunta como la probabilidad de que se tome el punto muestral $x_{i}$ y el punto muestral $y_{j}$, y está dada por la fracción de puntos muestrales en la celda $i,j$ dividida por la cantidad total de puntos muestrales\n$N$ y se denota como: $p\\left(Y=y_{j},X=x_{i}\\right)=p\\left(X=x_{i},Y=y_{j}\\right)=\\frac{n_{i,j}}{N} (1)$\n\n### Regla de la suma:\nLa probabilidad de que $X=x_{i}$ sin importar el valor de $Y$ viene dado por: $p\\left(X=x_{i}\\right)=p\\left(X=x_{i},Y=y_{1},\\ldots,y_{L}\\right)=\\sum_{j=1}^{L}p\\left(X=x_{i},Y=y_{j}\\right)=\\frac{c_{i}}{N}(2)$\n\nsimilar con la probabilidad de que $Y=y_{j}$ sin considerar la variable aleatoria $X$,  o sea, $p\\left(Y=y_{j}\\right)=\\frac{r_{j}}{N}$.\n\nA $p\\left(X=x_{i}\\right)$ también se le conoce como probabilidad marginal de $X$\n\n### Probabilidad condicional:\nLa probabilidad condicional se define como la probabilidad de escoger el punto muestral $Y=y_{j}$ dado que anteriormente se escogió el punto muestral $X=x_{i}$, o en otras palabras, dado que se escogió la columna $i$. Más formalmente se expresa como:\n$p\\left(Y=y_{j}|X=x_{i}\\right)=\\frac{n_{i,j}}{c_{i}}(3)$.\n\nObserve que la normalización se hace respecto a la cantidad de veces que se tomó el punto muestral $X=x_{i}$ sin importar $Y$.\n\n### Regla del producto de probabilidad:\nTomando las ecuaciones 1, 2 y 3 se puede reescribir: $p\\left(X=x_{i},Y=y_{j}\\right)=\\frac{n_{i,j}}{N}=\\frac{n_{i,j}}{c_{i}}\\cdot\\frac{c_{i}}{N}=p\\left(Y=y_{j}|X=x_{i}\\right)\\cdot p\\left(X=x_{i}\\right)$\n\nEn resumen se tienen dos reglas fundamentales de la teoría de la probabilidad:\n* **Regla de la suma**: $p\\left(X\\right)=\\sum\\limits_{Y}p\\left(X,Y\\right)$.\n* **Regla del producto**: $p\\left(X,Y\\right)=p\\left(Y|X\\right)\\cdot p\\left(X\\right)$."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BANluV_23tD0"
   },
   "source": "## Teorema de Bayes\n\nEl teorema de Bayes es de vital importancia para muchos de los algoritmos y técnicas del aprendizaje automático. Para deducirlo, nos basamos en la regla del producto y la **propiedad de simetría de la probabilidad conjunta** $p\\left(X,Y\\right)=p\\left(Y,X\\right)$, de forma que:\n\n$\np\\left(Y|X\\right)=\\frac{p\\left(X,Y\\right)}{p\\left(X\\right)}=\\frac{p\\left(Y,X\\right)}{p\\left(X\\right)}=\\frac{p\\left(X|Y\\right)\\cdot p\\left(Y\\right)}{p\\left(X\\right)}\n$\n\n$\n\\Rightarrow p\\left(Y|X\\right)=\\frac{p\\left(X|Y\\right)\\cdot p\\left(Y\\right)}{p\\left(X\\right)}\n$\n\nLa siguiente figura ilustra la probabilidad conjunta y marginal de $X$ y $Y$, graficadas en cada uno de los ejes, para el caso en el que $M=9$ y $L=2$.\n\n![](https://drive.google.com/uc?export=view&id=1NXuoni7v5q4hTuIl9aq39UVetEhFc31Z)\n\n\n\n\n* **Fórmula de Bayes**:\n\n  $\np\\left(H|D\\right)=\\frac{p\\left(D|H\\right)\\cdot p\\left(H\\right)}{p\\left(D\\right)}\n$\n\n  * **Verosimilitud (likelihood)**: Es la probabilidad $p(D|H)$ de observar la evidencia $D=d$ dado que la hipótesis $H=h$ es verdadera.\n\n  * **Probabilidad a priori o marginal**: Se refiere a la probabilidad $p\\left(H=h\\right)$ de que una hipótesis $H=h$ se de antes de observar uno o más datos $D=d$ al que puede estar condicionada la hipótesis.\n\n  * **Evidencia**: Es la probabilidad $p(D=d)$ de obtener la evidencia $D=d$\n\n  * **Probabilidad a posteriori:**\n  Corresponde a la probabilidad condicional\n  $p\\left(H=h|D=d\\right)$\n  la cual describe la probabilidad de que la hipótesis\n  $H=h$\n  se de, dados los datos\n  $D=d$.\n  \n  ### Ejemplo\n  \n  Retomando el ejemplo de las cajas de frutas, se puede definir una variable aleatoria $B$ que indique si la fruta es una manzana verde ($v$) o una naranja ($n$). Asimismo, una variable aleatoria $C$ que corresponda al color de la caja, donde tenemos las siguientes **probabilidades a priori**:\n  \n  $p\\left(C=r\\right)=0.4$\n\n  $p\\left(C=a\\right)=0.6$\n\n  Se le llama **probabilidad a priori, pues ningún evento ha sucedido** (escogencia de una fruta).\n  \n  Para establecer la **verosimilitud** o probabilidades condicionales, en este caso **no se usa un enfoque frecuentista** , puesto que no tenemos un historial de experimentos previos, en cambio se usa un **enfoque bayesiano** en el que las probabilidades se calculan según características conocidas de antemano para el experimento (en este caso la distribución de frutas por caja).   Es por esto que podemos entonces inferir que:\n\n  $\n  \\begin{array}{c}\n  p\\left(B=v|C=r\\right)=1/4\\\\\n  p\\left(B=n|C=r\\right)=3/4\\\\\n  p\\left(B=v|C=a\\right)=3/4\\\\\n  p\\left(B=n|C=a\\right)=1/4\n  \\end{array}\n  $\n\n![](https://drive.google.com/uc?export=view&id=1isFSFkup_XGfXd2cTUsFOwTr_09BbiDa)\n\n  Suponga ahora que se desea conocer la probabilidad de obtener una manzana verde  $p\\left(B=v\\right)$ sin importar la caja de la que viene, o en otras palabras, la probabilidad de obtener la **evidencia** $B=v$. Para ello se usa la regla de la suma y el producto:\n   $\n p\\left(B=v\\right)=\\sum\\limits_{C}p\\left(B=v|C\\right)\\cdot p\\left(C\\right)=p\\left(B=v|C=r\\right)p\\left(C=r\\right)+p\\left(B=v|C=a\\right)p\\left(C=a\\right)$\n\n  $\n  p\\left(B=v\\right)=\\frac{1}{4}\\times\\frac{4}{10}+\\frac{3}{4}\\times\\frac{6}{10}=\\frac{11}{20}$\n\n  Finalmente, si se desea conocer la probabilidad de que la caja escogida sea roja, dado que se sacó una naranja, se usa la **probabilidad a posteriori** $p\\left(C=r|B=n\\right)$. Se le llama probabilidad a posteriori, pues se obtiene luego de observar $B = n$, o bien, después del evento de sacar una naranja. Usando el teorema de Bayes, la probabilidad a posteriori se calcula como:\n\n  $\n  p\\left(C=r|B=n\\right)=\\frac{p\\left(B=n|C=r\\right)p\\left(C=r\\right)}{p\\left(B=n\\right)}=\\frac{3}{4}\\times\\frac{4}{10}\\times\\frac{20}{9}=\\frac{2}{3},\n  $\n\n\n\n  y usando el complemento podemos calcular $p\\left(C=a|B=n\\right)=1-2/3=1/3$ .\n  \n### Independencia de variables aleatorias:\n  \nSi dos variables aleatorias $X$ y $Y$son independientes, la probabilidad conjunta de que $X=x$ y $Y=y$ se puede expresar como:\n\n>>$p\\left(X,Y\\right)=p\\left(X\\right)p\\left(Y\\right).$\n\n\n\nPor la regla del producto que establece que\n$p\\left(X,Y\\right)=p\\left(Y|X\\right)\\cdot p\\left(X\\right)$, tenemos para las variables aleatorias $X$ e $Y$ independientes:\n\n>> $\np\\left(Y\\right)=p\\left(Y|X\\right),\n$\n\nlo cual se puede leer como que la probabilidad de que $Y=y$ es la misma independientemente del valor que tome $X$. En el ejemplo de las cajas con frutas, si ambas cajas tuviesen la misma cantidad de naranjas y manzanas, tendríamos que:\n\n>>$\n\\begin{array}{c}\np\\left(B=v|C=r\\right)=0.5\\\\\np\\left(B=n|C=r\\right)=0.5\\\\\np\\left(B=v|C=a\\right)=0.5\\\\\np\\left(B=n|C=a\\right)=0.5\n\\end{array}\n$\n\n y por lo tanto la probabilidad de sacar una naranja o una manzana verde sería independiente de la caja escogida, o en otras palabras se cumpliría que:\n\n >> $\np\\left(B\\right)=p\\left(B|C\\right),\n$\n\npues:\n\n>>$\np\\left(B=v\\right)=p\\left(B=v|C=r\\right)p\\left(C=r\\right)+p\\left(B=v|C=a\\right)p\\left(C=a\\right)=0.5\\times0.4+0.5\\times0.6=0.5\n$"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G5mBVN4Z0q3p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bdd5b23d-d4d8-4d7f-c739-c9e88365cfbe"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Crear una funcion que genere puntos muestrales aleatorios dadas las dos probabilidades de manzana y\n",
    "#naranja (como parametro)\n",
    "\n",
    "\n",
    "# Definir las probabilidades a priori\n",
    "prob_manzana_roja = 1/4\n",
    "prob_naranja_roja = 3/4\n",
    "prob_manzana_azul = 3/4\n",
    "prob_naranja_azul = 1/4\n",
    "prob_roja = 0.4\n",
    "prob_azul= 0.6\n",
    "\n",
    "#1\n",
    "\n",
    "def generar_observacion():\n",
    "    caja = np.random.choice(['roja', 'azul'], p=[prob_roja, prob_azul])\n",
    "    if caja == 'roja':\n",
    "        fruta = np.random.choice(['manzana', 'naranja'], p=[prob_manzana_roja, prob_naranja_roja])\n",
    "    else:\n",
    "        fruta = np.random.choice(['manzana', 'naranja'], p=[prob_manzana_azul, prob_naranja_azul])\n",
    "    return caja, fruta\n",
    "\n",
    "\n",
    "\n",
    "# 2. Crear una función que genere muestras a partir del paso 1 (el tamaño de la caja se recibe por parametro) Cada caja es un conjunto de muestras\n",
    "\n",
    "def generar_muestra(num_muestras,num_obseraciones):\n",
    "    muestras = []\n",
    "    for i in range(num_muestras):\n",
    "        muestra = [generar_observacion() for _ in range(num_obseraciones)]\n",
    "        muestras.append(muestra)\n",
    "    return muestras\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 3. Luego de generar 3 muestras de diferentes tamanios y con diferentes probabilidades a priori,\n",
    "# # Cree funciones para calcular la probabilidad de obtener cualquier fruta (evidencia) y la probabilidad a posteriori de\n",
    "# #cualquier caja dado que se escogió una fruta en particular\n",
    "\n",
    "\n",
    "def probabilidad_marginal(muestras, fruta):\n",
    "    total_muestras = sum(len(muestra) for muestra in muestras)\n",
    "    contador_muestra = sum(1 for muestra in muestras for _, m in muestra if m == fruta)\n",
    "    return contador_muestra / total_muestras\n",
    "\n",
    "def probabilidad_condicional(muestras, fruta, color_caja):\n",
    "    total_muestras_en_caja = sum(1 for muestra in muestras for c, _ in muestra if c == color_caja)\n",
    "    contador_muestra_en_caja = sum(1 for muestra in muestras for c, m in muestra if m == fruta and c == color_caja)\n",
    "    return contador_muestra_en_caja / total_muestras_en_caja\n",
    "\n",
    "def probabilidad_a_posteriori(muestras, fruta, color_caja):\n",
    "    if color_caja == 'roja':\n",
    "        prob_a_priori = prob_caja_roja\n",
    "    else:\n",
    "        prob_a_priori = prob_caja_azul\n",
    "\n",
    "    prob_condicional = probabilidad_condicional(muestras, fruta, color_caja)\n",
    "    prob_marginal_muestra = probabilidad_marginal(muestras, fruta)\n",
    "    return (prob_condicional * prob_a_priori) / prob_marginal_muestra\n",
    "\n",
    "\n",
    "num_observaciones = 100\n",
    "num_muestras = 10\n",
    "\n",
    "muestras = generar_muestra(num_muestras, num_observaciones)\n",
    "\n",
    "\n",
    "\n",
    "# Calcular probabilidad marginal de una manzana y una naranja\n",
    "prob_manzana = probabilidad_marginal(muestras, 'manzana')\n",
    "prob_naranja = probabilidad_marginal(muestras, 'naranja')\n",
    "\n",
    "print(f\"Probabilidad marginal de manzana: {prob_manzana:.2f}\")\n",
    "print(f\"Probabilidad marginal de naranja: {prob_naranja:.2f}\")\n",
    "\n",
    "# Calcular probabilidad a posteriori de que la muestra sea de la caja roja o azul\n",
    "fruta = 'manzana'\n",
    "prob_roja = probabilidad_a_posteriori(muestras, fruta, 'roja', prob_roja, prob_azul)\n",
    "prob_azul = probabilidad_a_posteriori(muestras, fruta, 'azul', prob_roja, prob_azul)\n",
    "\n",
    "print(f\"Probabilidad a posteriori de caja roja dado {fruta}: {prob_roja:.2f}\")\n",
    "print(f\"Probabilidad a posteriori de caja azul dado {fruta}: {prob_azul:.2f}\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Probabilidad marginal de verde: 0.55\n",
      "Probabilidad marginal de naranja: 0.45\n",
      "Probabilidad a posteriori de caja roja dado verde: 0.18\n",
      "Probabilidad a posteriori de caja azul dado verde: 0.80\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L57SQJzT2WcY"
   },
   "source": [
    "## Función de distribución de probabilidad acumulativa (CDF)\n",
    "\n",
    "La función de distribución de probabilidad acumulativa $F_X(x)$ indica la probabilidad de que una variable aleatoria $X$ tome un valor menor o igual a un número real $x$. En otras palabras $F_X(x) = p(X \\leq x)$. Para simplificar la notación de aquí en adelante nos referiremos a $F_X(x)$ simplemente como $F(x)$. La función de distribución de probabilidad acumulativa presenta las siguientes propiedades:\n",
    "1. $F(-\\infty)=0$\n",
    "\n",
    "2. $F(\\infty)=1$\n",
    "\n",
    "3. $0 \\leq F(x) \\leq 1$\n",
    "\n",
    "4. $ F(x_1) \\leq F(x_2)$ si $x_1 \\leq x_2$\n",
    "\n",
    "5. $p(x_1 \\leq X \\leq x_2) = F(x_2) - F(x_1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzhOzfsim15g"
   },
   "source": [
    "## Función de densidad de probabilidad (PDF)\n",
    "\n",
    "La función de densidad de probabilidad, denotada por $f(x)$, está definida por la derivada de la función de distribución:\n",
    "\n",
    "$f(x)= \\frac{dF(x)}{dx}$\n",
    "\n",
    "\n",
    "![](https://www.vosesoftware.com/riskwiki/images/image15_400.gif)\n",
    "\n",
    "(Explicación gráfica:https: //www.youtube.com/watch?v=Q0auB05R3Vs)\n",
    "\n",
    "\n",
    "Toda función de densidad de probabilidad, definida para una variable aleatoria continua, debe cumplir las siguientes condiciones básicas:\n",
    "\n",
    "1. $f\\left(x\\right)\\geq 0$\n",
    "\n",
    "2. $\\int_\\limits{-\\infty}^{\\infty}f\\left(x\\right)\\textrm{d}x=1$\n",
    "\n",
    "3. $F(x) = \\int_\\limits{-\\infty}^{x}f\\left(u\\right)\\textrm{d}u$\n",
    "\n",
    "4. $p(x_1 \\leq X \\leq x_2) = \\int_\\limits{x_1}^{x_2}f\\left(x\\right)\\textrm{d}x.$\n",
    "\n",
    "En el caso de variables aleatorias discretas, se le suele llamar **función de masa de probabilidad**, y las propiedades se reescriben como:\n",
    "\n",
    "1. $f\\left[x\\right]\\geq 0$\n",
    "\n",
    "2. $\\sum \\limits_x f\\left[x\\right]=1$\n",
    "\n",
    "3. $F[x] = \\sum_\\limits{x_i \\leq x}^{}f\\left[x_i\\right]$\n",
    "\n",
    "4. $p[x_1 \\leq X \\leq x_2] = \\sum_\\limits{x_1 \\leq x_i \\leq x_2}^{}f\\left[x_i\\right].$\n",
    "\n",
    "\n",
    "La siguiente figura presenta múltiples histogramas creados a partir de muestras tomadas de diferentes funciones de densidad probabilística.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1AQH2wfCwXlxiR0zLthYcLcou0ioZQ-Ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6W4OdsCVnZK-"
   },
   "source": [
    "### Esperanza de una variable aleatoria\n",
    "\n",
    "El cálculo de momentos estadísticos es una operación muy utilizada en el reconocimiento de patrones, pues permite reducir la dimensionalidad de los datos, obteniendo características importantes como el valor esperado o la varianza.\n",
    "\n",
    "Se define entonces la **esperanza, valor esperado o media** de una variable aleatoria $X$, como la sumatoria de cada valor $x$ que puede tomar $X$ pesado por su probabilidad . A continuación se muestra el cálculo de la esperanza para variables aleatorias continuas y discretas, respectivamente:\n",
    "\n",
    "$\n",
    "\\mu_{X}=\\mathbb{E}\\left(X\\right)=\\int \\limits_{-\\infty}^{\\infty} x\\,f\\left(x\\right)\\textrm{d}x\n",
    "$\n",
    "\n",
    "$\\mu_{X}=\\mathbb{E}\\left[X\\right]=\\sum \\limits_x x\\,f\\left[x\\right]\n",
    "$\n",
    "\n",
    "En el caso de contar únicamente con un número finito de $N$ observaciones tomadas de la distribución de probabilidad de $X$, se puede **aproximar** la esperanza como:\n",
    "\n",
    ">>$\n",
    "\\mathbb{E}\\left[X\\right]\\cong\\frac{1}{N}\\sum \\limits _{i=1}^{N}x_i\n",
    "$\n",
    "\n",
    "Desde un punto de vista frecuentista, la aproximación de $\\mathbb{E}\\left[X\\right]$ mejora a medida que $N\\rightarrow\\infty$.\n",
    "\n",
    "Las siguientes son propiedades de la esperanza:\n",
    "\n",
    "* Si $a$ es un escalar, tal que $a\\in\\mathbb{R}$, se tiene que:\n",
    "$\\mathbb{E}\\left[a\\right]=a$\n",
    "* Homogeneidad:\n",
    "$\\mathbb{E}\\left[a\\,X\\right]=a\\,\\mathbb{E}\\left[X\\right].$\n",
    "*Superposición:\n",
    "$\\mathbb{E}\\left[g\\left(X\\right)+h\\left(X\\right)\\right]=\\mathbb{E}\\left[g\\left(X\\right)\\right]+\\mathbb{E}\\left[h\\left(X\\right)\\right].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Esqmw6hqmFg"
   },
   "source": [
    "## Ejercicio Bayes\n",
    "\n",
    "Una prueba de diagnóstico para una enfermedad es tal que (correctamente) detecta la enfermedad en\n",
    "90% de los individuos que en realidad tienen la enfermedad. También, si una persona no tiene la enfermedad, la prueba reportará que él o ella no la tiene con probabilidad 0.9. Sólo 1% de la población tiene la enfermedad en cuestión. Si una persona es seleccionada al azar de la población y la prueba de diagnóstico indica que tiene la enfermedad, ¿cuál es la probabilidad condicional de que tenga, en realidad,\n",
    "la enfermedad?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-fced5NPVRi3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fed3cb08-ca65-491f-b3f2-d4df0f834774"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "#T: prueba\n",
    "#D: enfermedad\n",
    "\n",
    "# Probabilidades conocidas\n",
    "P_enfermedad = 0.01\n",
    "P_no_enfermedad = 0.99\n",
    "P_prueba_positiva_dado_enfermedad = 0.90\n",
    "P_prueba_negativa_dado_no_enfermedad = 0.90\n",
    "\n",
    "# Calcular P(T | not D)\n",
    "P_prueba_positiva_dado_no_enfermedad = 1 - P_prueba_negativa_dado_no_enfermedad\n",
    "\n",
    "# Calcular P(T) usando la ley de probabilidades totales\n",
    "P_prueba_positiva = (P_prueba_positiva_dado_enfermedad * P_enfermedad) + (P_prueba_positiva_dado_no_enfermedad * P_no_enfermedad)\n",
    "\n",
    "# Aplicar el Teorema de Bayes para encontrar P(D | T)\n",
    "P_enfermedad_dado_prueba_positiva = (P_prueba_positiva_dado_enfermedad * P_enfermedad)/ P_prueba_positiva\n",
    "\n",
    "print(P_enfermedad_dado_prueba_positiva)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.08333333333333336\n"
     ]
    }
   ]
  }
 ]
}