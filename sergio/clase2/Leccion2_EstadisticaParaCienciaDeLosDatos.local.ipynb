{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPD8sqQXo3_P"
      },
      "source": [
        "# Estadística para Ciencia de los Datos - Lección 2\n",
        "\n",
        "Autor: Saúl Calderón, Juan Esquivel, Jorge Castro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUrAWwfyzIQ_"
      },
      "source": [
        "# Probabilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJuoKD9Wogla"
      },
      "source": [
        "## Varianza\n",
        "\n",
        "La varianza de una variable aleatoria $X$ es una medida de la \"concentración\" o dispersión de los datos alrededor de la media. Formalmente la varianza de una variable aleatoria $X$ se define como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma_{X}^{2}\\textrm{=var}\\left[X\\right]=\\mathbb{E}\\left[\\left(X-\\mathbb{E}\\left[X\\right]\\right)^{2}\\right],\n",
        "\\end{equation}\n",
        "\n",
        "lo cual equivale a (por la linealidad de la esperanza y su propiedad de equivalencia en constantes):\n",
        "\n",
        "\\begin{equation}\n",
        "\\\\\n",
        "\\mathbb{E}\\left[\\left(X-\\mathbb{E}\\left[X\\right]\\right)^{2}\\right]=\\mathbb{E}\\left[\\left(X^{2}-2X\\mathbb{E}\\left[X\\right]+\\mathbb{E}\\left[X\\right]^{2}\\right)\\right]\n",
        "\\\\\n",
        "\\Rightarrow\\textrm{var}\\left[X\\right]=\\left(\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}\\left[2X\\mathbb{E}\\left[X\\right]\\right]+\\mathbb{E}\\left[\\mathbb{E}\\left[X\\right]^{2}\\right]\\right)\n",
        "\\\\\n",
        "\\Rightarrow\\textrm{var}\\left[X\\right]=\\mathbb{E}\\left[X^{2}\\right]-2\\mathbb{E}\\left[X\\right]^{2}+\\mathbb{E}\\left[X\\right]^{2}\n",
        "\\\\\n",
        "\\Rightarrow\\textrm{var}\\left[X\\right]=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}\\left[X\\right]^{2}\n",
        "\\\\\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Propiedades de la varianza:\n",
        "\n",
        "* Si $a$ es un escalar, tal que $a\\in\\mathbb{R}$, se tiene que: $\\textrm{Var}\\left[a\\right]=0$.\n",
        "*Multiplicación por escalar de la entrada:\n",
        "$\\textrm{Var}\\left[a\\,X\\right]=a^{2}\\textrm{Var}\\left[X\\right]$, $\\forall a\\in\\mathbb{R}$.\n",
        "\n",
        "Si únicamente se tiene un estimado de la media $\\hat \\mu$, y se toma la suposición de que los datos provienen de una distribución normal, la varianza se estima de la siguiente forma:\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma_{X}^{2}\\cong\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(x_i-\\hat \\mu_{X}\\right)^{2}. (1)\n",
        "\\end{equation}\n",
        "\n",
        "El estimador anterior se dice que no es \"sesgado\" si la media real no es conocida. Si la media real de la población es conocida, o bien se tiene un N muy grande, se normaliza por\n",
        "$N$ ( puede ver la demostración de lo anterior  [aquí](http://www.visiondummy.com/2014/03/divide-variance-n-1/)  \"http://www.visiondummy.com/2014/03/divide-variance-n-1/\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dIg_RjvPPZx"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrvHLoQotIU"
      },
      "source": [
        "### Ejemplo\n",
        "\n",
        "Calcule la media y la varianza de una variable aleatoria uniforme $X$ con una función de densidad $f\\left(x\\right)=1,\\:\\forall x\\in\\left[0,1\\right],\\; f(x)=0\\:\\textrm{de otro modo}$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}\\left[X\\right]=\\int_{-\\infty}^{\\infty}x\\,f\\left(x\\right)\\,\\textrm{d}x=\\int_{0}^{1}x\\,\\textrm{d}x=\\frac{1}{2}\n",
        "\\\\\\mathbb{E}\\left[X^{2}\\right]=\\int_{-\\infty}^{\\infty}x^{2}\\,f\\left(x\\right)\\,\\textrm{d}x=\\int_{0}^{1}x^{2}\\,\\textrm{d}x=\\frac{1}{3}\n",
        "\\\\\n",
        "\\textrm{Var}\\left[X\\right]=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}\\left[X\\right]^{2}=\\frac{1}{3}-\\frac{1}{4}=\\frac{1}{12}\n",
        "\\\\\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5-QvSmTo3Id"
      },
      "source": [
        "## Covarianza\n",
        "\n",
        "Para dos variables aleatorias $X$ e $Y$ se define la covarianza como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma_{X,Y}=\\textrm{cov}\\left(X,Y\\right)=\\mathbb{E}\\left[\\left(X-\\mathbb{E}\\left[X\\right]\\right)\\left(Y-\\mathbb{E}\\left[Y\\right]\\right)\\right]\n",
        "\\end{equation}\n",
        "\n",
        "y mide la variación conjunta de tales variables aleatorias. Para el caso de contar con $N$ observaciones de ambas variables, la covarianza estaría estimada por:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma_{X,Y}\\cong\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(x_i-\\overline{\\mu}_{X}\\right)\\left(y_i-\\overline{\\mu}_{Y}\\right),\n",
        "\\end{equation}\n",
        "\n",
        "donde $\\overline{\\mu}_{X}$ corresponde a la media muestral de $X$ y $\\overline{\\mu}_{Y}$ corresponde a la media muestral de $Y$. La matriz de covarianza para $n$ variables aleatorias $X_{1},X_{2},\\ldots,X_{n}$ se define como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma=\\begin{bmatrix}\\mathbb{E}\\left[\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\right] & \\ldots & \\mathbb{E}\\left[\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\right]\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\mathbb{E}\\left[\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\right] & \\ldots & \\mathbb{E}\\left[\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\right]\n",
        "\\end{bmatrix},\n",
        "\\end{equation}\n",
        "\n",
        "observe que en la diagonal de la matriz $\\Sigma$ (entrada $\\Sigma_{i,i}$) se tiene que\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}\\left[\\left(X_{i}-\\mathbb{E}\\left[X_{i}\\right]\\right)\\left(X_{i}-\\mathbb{E}\\left[X_{i}\\right]\\right)\\right]=\\sigma_{X_{i}}^{2},\n",
        "\\end{equation}\n",
        "\n",
        "por lo que entonces la matriz de covarianza se puede reescribir como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma=\\begin{bmatrix}\\sigma_{X_{1}}^{2} & \\ldots & \\mathbb{E}\\left[\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\right]\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\mathbb{E}\\left[\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\right] & \\ldots & \\sigma_{X_{n}}^{2}\n",
        "\\end{bmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        "Además, la matriz de covarianza $\\Sigma$ presenta la propiedad de ser simétrica, puesto que\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}\\left[\\left(X_{i}-\\mathbb{E}\\left[X_{i}\\right]\\right)\\left(X_{j}-\\mathbb{E}\\left[X_{j}\\right]\\right)\\right]=\\mathbb{E}\\left[\\left(X_{j}-\\mathbb{E}\\left[X_{j}\\right]\\right)\\left(X_{i}-\\mathbb{E}\\left[X_{i}\\right]\\right)\\right]\\Rightarrow\\Sigma_{X_{i},X_{j}}=\\Sigma_{X_{j},X_{i}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yixxkd9pA1f"
      },
      "source": [
        "### Ejemplo\n",
        "\n",
        "Suponga que se desea encontrar la matriz de covarianza para tres variables aleatorias $X_{1},X_{2}$ y $X_{3}$, para las cuales se han recabado los siguientes arreglos de $N=4$ observaciones:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{c}\n",
        "h_{1}=\\begin{bmatrix}2 & 4 & 6 & 8\\end{bmatrix}\\\\\n",
        "h_{2}=\\begin{bmatrix}4 & 8 & 12 & 16\\end{bmatrix}\\\\\n",
        "h_{3}=\\begin{bmatrix}12 & 10 & 5 & 9\\end{bmatrix}\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "Las observaciones se pueden redefinir como vectores de longitud igual al número de variables aleatorias:\n",
        "\n",
        "\\begin{equation}\n",
        "U=\\left\\{ \\vec{u}_{1},\\vec{u}_{2},\\vec{u}_{3},\\vec{u}_{4}\\right\\} =\\begin{bmatrix}| & | & | & |\\\\\n",
        "\\vec{u}_{1} & \\vec{u}_{2} & \\vec{u}_{3} & \\vec{u}_{4}\\\\\n",
        "| & | & | & |\n",
        "\\end{bmatrix}=\\begin{bmatrix}2 & 4 & 6 & 8\\\\\n",
        "4 & 8 & 12 & 16\\\\\n",
        "12 & 10 & 5 & 9\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "con $\\vec{u}_{i}\\in\\mathbb{R}^{3}$, donde cada dimensión es una variable aleatoria, y $U \\in\\mathbb{R}^{3\\times4}$.\n",
        "\n",
        "Observe en estos datos, que las filas 1 y 2 son combinación lineal para todas las observaciones, por lo que la covarianza de ambas dimensiones debe ser alta, no así la fila 1 con la 3 o la 2 con la 3.\n",
        "\n",
        "Se procede entonces a calcular las entradas $\\Sigma_{X_{1},X_{2}}$, $\\Sigma_{X_{1},X_{3}}$ y $\\Sigma_{X_{2},X_{3}}$, además de los valores de la diagonal $\\sigma_{X_{1}}^{2}$, $\\sigma_{X_{2}}^{2}$ y $\\sigma_{X_{3}}^{2}$, teniendo en cuenta que $\\overline{\\mu}_{X_{1}}=5$, $\\overline{\\mu}_{X_{2}}=10$ y $\\overline{\\mu}_{X_{3}}=9$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{c}\n",
        "\\Sigma_{X_{1},X_{2}}=\\frac{1}{4-1}\\left(\\left(5-2\\right)\\left(10-4\\right)+\\left(5-4\\right)\\left(10-8\\right)+\\left(5-6\\right)\\left(10-12\\right)+\\left(5-8\\right)\\left(10-16\\right)\\right)\\\\\n",
        "\\Sigma_{X_{1},X_{3}}=\\frac{1}{4-1}\\left(\\left(5-2\\right)\\left(9-12\\right)+\\left(5-4\\right)\\left(9-10\\right)+\\left(5-6\\right)\\left(9-5\\right)+\\left(5-8\\right)\\left(9-9\\right)\\right)\\\\\n",
        "\\Sigma_{X_{2},X_{3}}=\\frac{1}{4-1}\\left(\\left(10-4\\right)\\left(9-12\\right)+\\left(10-8\\right)\\left(9-10\\right)+\\left(10-12\\right)\\left(9-5\\right)+\\left(10-16\\right)\\left(9-9\\right)\\right)\\\\\n",
        "\\sigma_{X_{1}}^{2}=\\frac{1}{4-1}\\left(\\left(5-2\\right)^{2}+\\left(5-4\\right)^{2}+\\left(5-6\\right)^{2}+\\left(5-8\\right)^{2}\\right)\\\\\n",
        "\\sigma_{X_{2}}^{2}=\\frac{1}{4-1}\\left(\\left(10-4\\right)^{2}+\\left(10-8\\right)^{2}+\\left(10-12\\right)^{2}+\\left(10-16\\right)^{2}\\right)\\\\\n",
        "\\sigma_{X_{3}}^{2}=\\frac{1}{4-1}\\left(\\left(9-12\\right)^{2}+\\left(9-10\\right)^{2}+\\left(9-5\\right)^{2}+\\left(9-9\\right)^{2}\\right)\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "lo cual desarrollado corresponde a:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{c}\n",
        "\\Sigma_{X_{1},X_{2}}=\\frac{1}{3}\\left(3\\cdot6+1\\cdot2+-1\\cdot-2+-3\\cdot-6\\right)=\\frac{40}{3}=13.333\\\\\n",
        "\\Sigma_{X_{1},X_{3}}=\\frac{1}{3}\\left(3\\cdot-3+1\\cdot-1+-1\\cdot4+-3\\cdot0\\right)=-\\frac{14}{3}=-4.667\\\\\n",
        "\\Sigma_{X_{2},X_{3}}=\\frac{1}{3}\\left(6\\cdot-3+2\\cdot-1+-2\\cdot4+-6\\cdot0\\right)=-\\frac{28}{3}=-9.333\\\\\n",
        "\\sigma_{X_{1}}^{2}=\\frac{1}{3}\\left(9+1+1+9\\right)=\\frac{20}{3}=6.667\\\\\n",
        "\\sigma_{X_{2}}^{2}=\\frac{1}{3}\\left(36+4+4+36\\right)=\\frac{80}{3}=26.667\\\\\n",
        "\\sigma_{X_{3}}^{2}=\\frac{1}{3}\\left(9+1+16+0\\right)=\\frac{14}{3}=8.667.\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "Por lo que se obtiene la matriz de covarianza:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma=\\begin{bmatrix}\\frac{20}{3} & \\frac{40}{3} & -\\frac{14}{3}\\\\\n",
        "\\frac{40}{3} & \\frac{80}{3} & -\\frac{28}{3}\\\\\n",
        "-\\frac{14}{3} & -\\frac{28}{3} & \\frac{14}{3}\n",
        "\\end{bmatrix}=\\begin{bmatrix}6.667 & 13.333 & -4.667\\\\\n",
        "13.333 & 26.666 & -9.333\\\\\n",
        "-4.667 & -9.333 & 8.667\n",
        "\\end{bmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        "Observe que la covarianza más alta es $\\Sigma_{X_{1},X_{2}}$ pues los datos de las variables aleatorias\n",
        "$X_{1}$ y $X_{2}$ siempre covarían positivamente para todas las observaciones. En los otros dos casos, donde la covarianza es negativa, ello denota que cuando en una dimensión los datos varían en una dirección, en la otra los datos varían en dirección contraria.\n",
        "\n",
        "También la matriz de covarianza se puede escribir, para una muestra de $m$ observaciones de $n$ variables aleatorias  $\\left\\{ \\vec{x}_{1},\\vec{x}_{2},\\ldots,\\vec{x}_{m}\\right\\} $ con $\\vec{x}_{i}\\in\\mathbb{R}^{n\\times1}$,  como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma=\\frac{1}{m-1}\\sum_{i=1}^{m}\\left(\\vec{x}_{i}-\\vec{\\mu}\\right)\\left(\\vec{x}_{i}-\\vec{\\mu}\\right)^{T}\n",
        "\\end{equation}\n",
        "\n",
        "donde $\\vec{\\mu}\\in\\mathbb{R}^{n\\times1}$ es el vector de medias de las $n$ variables aleatorias. Al igual que con la varianza, en caso de conocerse la media real de las variables aleatorias o contar con una muestra $m$ muy grande se puede sustituir el factor $\\frac{1}{m-1}$ por $\\frac{1}{m}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxDEeNW4paEG"
      },
      "source": [
        "## Coeficiente de correlación de Pearson\n",
        "\n",
        "En el ejemplo anterior, se concluyó que las variables aleatorias\n",
        "$X_{1}$ y $X_{2}$ covarían fuertemente, sin embargo es difícil interpretar el grado en el que lo hacen pues la covarianza se expresa en unidades que varían de acuerdo a los datos. La matriz de Pearson denotada como $\\rho$, permite observar el grado de covarianza de las variables aleatorias, en el intervalo de $[-1, 1]$, o dicho en otras palabras,  normaliza la covarianza. Para dos variables aleatorias $X_{i}$ y $X_{j}$, el coeficiente de correlación Pearson $\\rho_{X_{i},X_{j}}$ normaliza la covarianza de ambas variables al dividirla por el producto de la desviación estandar de ambas variables aleatorias:\n",
        "\n",
        "\\begin{equation}\n",
        "\\rho_{X_{i},X_{j}}=\\frac{\\textrm{cov}\\left(X_{i},X_{j}\\right)}{\\sigma_{X_{i}}\\sigma_{X_{j}}}\n",
        "\\end{equation}\n",
        "\n",
        "definiendo así la matriz de correlación de Pearson:\n",
        "\n",
        "\\begin{equation}\n",
        "\\rho=\\begin{bmatrix}\\frac{\\mathbb{E}\\left[\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\right]}{\\sigma_{X_{1}}\\sigma_{X_{1}}} & \\ldots & \\frac{\\mathbb{E}\\left[\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\right]}{\\sigma_{X_{1}}\\sigma_{X_{n}}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\mathbb{E}\\left[\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\right]}{\\sigma_{X_{n}}\\sigma_{X_{1}}} & \\ldots & \\frac{\\mathbb{E}\\left[\\left(X_{n}-\\mathbb{E}\\left[X_{n}\\right]\\right)\\left(X_{1}-\\mathbb{E}\\left[X_{1}\\right]\\right)\\right]}{\\sigma_{X_{n}}\\sigma_{X_{n}}}\\ldots\n",
        "\\end{bmatrix},\n",
        "\\end{equation}\n",
        "\n",
        "donde los valores de la diagonal están dados por\n",
        "\n",
        "\\begin{equation}\n",
        "\\rho_{X_{i},X_{i}}=\\frac{\\mathbb{E}\\left[\\left(X_{i}-\\mathbb{E}\\left[X_{i}\\right]\\right)\\left(X_{i}-\\mathbb{E}\\left[X_{i}\\right]\\right)\\right]}{\\sigma_{X_{i}}\\sigma_{X_{i}}}=1\n",
        "\\end{equation}\n",
        "\n",
        "En la siguiente figura se muestra como se comporta el coeficiente de Pearson ante distintas condiciones de dos variables aleatorias $X_{1}$ (eje x) y $X_{2}$ (eje y) para los cuales se grafican los datos:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1bp6bFNb6C_rSUqeuRGT9DSAmRMGdAttu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNdCGSvOpoW-"
      },
      "source": [
        "## La función de densidad Gaussiana\n",
        "\n",
        "La función de densidad Gaussiana es la función de densidad más utilizada en el análisis de datos y el reconocimiento de patrones, pues muchos fenómenos aleatorios naturales (como por ejemplo el peso de las personas en una cierta edad, características físicas en animales y plantas, etc.) se modelan de forma satisfactoria con una función de densidad Gaussiana.\n",
        "\n",
        "La función de densidad Gaussiana es un modelo con dos parámetros: la media $\\mu$ y la dispersión $\\sigma$. Para el caso de una dimensión en la que el codominio está dado por $x\\in\\mathbb{R}$, se tiene que la función de densidad de probabilidad está dada por:\n",
        "\n",
        "\\begin{equation}\n",
        "f\\left(x\\right)=\\mathcal{N}\\left(x|\\mu,\\sigma\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(x-\\mu\\right)^{2}\\right),\n",
        "\\end{equation}\n",
        "\n",
        "la cual describe la distribución de los datos según un modelo Gaussiano con parámetros $\\theta=\\left(\\mu,\\sigma\\right)$. El coeficiente $\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}$ normaliza la función de densidad y garantiza el área bajo la curva sea de 1:\n",
        "\n",
        "\\begin{equation}\n",
        "\\int_{-\\infty}^{\\infty}\\mathcal{N}\\left(x|\\mu,\\sigma\\right)\\textrm{d}x=1.\n",
        "\\end{equation}\n",
        "\n",
        "Además:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{N}\\left(x|\\mu,\\sigma\\right)>0.\n",
        "\\end{equation}\n",
        "\n",
        "La esperanza de una variable aleatoria $X$ caracterizada por una distribución normal $\\mathcal{N}\\left(x|\\mu,\\sigma\\right)$ viene dada por la media, puesto que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}\\left[X\\right]=\\int_{-\\infty}^{\\infty}x\\,\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(x-\\mu\\right)^{2}\\right)\\textrm{d}x=\\mu,\n",
        "\\end{equation}\n",
        "\n",
        "y la esperanza de la variable aleatoria al cuadrado está dada por:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}\\left[X^{2}\\right]=\\int_{-\\infty}^{\\infty}x^{2}\\,\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(x^2-\\mu\\right)^{2}\\right)\\textrm{d}x=\\mu^{2}+\\sigma^{2},\n",
        "\\end{equation}\n",
        "\n",
        "por lo cual se deduce que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\textrm{var}\\left[X\\right]=\\mathbb{E}\\left[X^{2}\\right]-\\mathbb{E}\\left[X\\right]^{2}=\\sigma^{2}.\n",
        "\\end{equation}\n",
        "\n",
        "El valor $x$ que maximiza la función de densidad de probabilidad Gaussiana, coincide con la **media**, **mediana** y **moda**, como ilustra la la siguente figura\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1lHekBg7C-cdYT8vB0egQ5w8UiVjAPjHi)\n",
        "\n",
        "![](image1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbAHhf2zp_py"
      },
      "source": [
        "## Función de verosimilitud\n",
        "\n",
        "Suponga ahora que se cuenta con un arreglo $\\vec{h}=\\left[h_{1},\\ldots,h_{M}\\right]$ de $M$ observaciones discretas sobre el dominio de valores que puede tomar la variable aleatoria $X$.\n",
        "Se supone además que tales observaciones son **independientes e idénticamente distribuidas (i.i.d.)**, o sea, que son generadas independientemente y con una misma distribución Gaussiana cuyos parámetros $\\mu$ y $\\sigma$ son desconocidos y quisieramos determinarlos a partir de nuestro set de datos, en este caso el arreglo $\\vec{h}$. Para denotar la probabilidad conjunta de que todo el arreglo $\\vec{h}$ haya sido generado por una variable aleatoria Gaussiana, se escribe la probabilidad condicional $p\\left(\\vec{h}|\\mu,\\sigma\\right)=p\\left(h_{1},h_{2},\\ldots,h_{m}|\\mu,\\sigma\\right)$, la cual, en el caso de que las observaciones sean independientes, corresponde a la multiplicación de las probabilidades individuales de que cada observación, a lo que se le conoce como la **función de verosimilitud**:\n",
        "\n",
        "\\begin{equation}\n",
        "p\\left(\\vec{h}|\\mu,\\sigma\\right)=\\prod_{n=1}^{M}\\mathcal{N}\\left(h_{n}|\\mu,\\sigma\\right) \\enspace (6)\n",
        "\\end{equation}\n",
        "\n",
        "Dado que lo único que conocemos son los datos en el arreglo $\\vec{h}$, es necesario encontrar los parámetros $\\mu$ y $\\sigma$ que maximicen la función de verosimilitud $p\\left(\\vec{h}|\\mu,\\sigma\\right)$, por lo que entonces, para todos los puntos $h_{1},\\ldots h_{M}$ la multiplicatoria debe ser lo máximo posible, lo cual corresponde a lo ilustrado en la siguiente figura\n",
        "\n",
        "![](image2.png)\n",
        "\n",
        "Para facilitar el cálculo matemático de la maximización de la función de verosimilitud se **utiliza el logaritmo natural** de la función de verosimilitud. Dado que el logaritmo es una función monotónicamente creciente, como se muestra en la siguiente figura:\n",
        "\n",
        "![](image3.png)\n",
        "\n",
        "\n",
        "la maximización del logaritmo de una función es equivalente a la maximización de la propia función. El logaritmo natural es usual en cálculos que involucran probabilidades, pues evita el problema del \"underflow\" que resulta de calcular el producto de muchos números de magnitud menor que uno. Las siguientes son propiedades del logaritmo natural:\n",
        "\n",
        "* $\\ln\\left(x\\cdot y\\right)=\\ln\\left(x\\right)+\\ln\\left(y\\right)$\n",
        "* $\\ln\\left(e\\right)=1$\n",
        "* $\\ln\\left(x^{n}\\right)=n\\,\\ln\\left(x\\right)$\n",
        "* $\\ln\\left(\\frac{x}{y}\\right)=\\ln\\left(x\\right)-\\ln\\left(y\\right)$\n",
        "* $\\ln\\left(1\\right)=0$\n",
        "* $\\ln\\left(x\\right)<\\ln\\left(y\\right),\\forall\\,0<x<y$\n",
        "* $\\frac{d}{dx}\\ln\\left(x\\right)=\\frac{1}{x}$\n",
        "\n",
        "Calculando entonces el logaritmo natural de la función de verosimilitud y usando sus propiedades se obtiene la siguiente expresión simplificada:\n",
        "\n",
        "\\begin{equation}\n",
        "\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=\\ln\\left(\\prod_{n=1}^{M}\\mathcal{N}\\left(h_{n}|\\mu,\\sigma\\right)\\right)\n",
        "\\\\\n",
        "\\Rightarrow\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=\\sum_{n=1}^{M}\\ln\\left(\\left(2\\pi\\sigma^{2}\\right)^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(h_{n}-\\mu\\right)^{2}\\right)\\right)\n",
        "\\\\\n",
        "\\Rightarrow\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=M\\,\\ln\\left(\\left(2\\pi\\sigma^{2}\\right)^{-\\frac{1}{2}}\\right)+\\sum_{n=1}^{M}\\ln\\left(\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left(h_{n}-\\mu\\right)^{2}\\right)\\right)\n",
        "\\\\\n",
        "\\Rightarrow\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=-\\frac{M}{2}\\ln\\left(2\\pi\\sigma^{2}\\right)+-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{M}\\left(h_{n}-\\mu\\right)^{2}\n",
        "\\\\\n",
        "\\Rightarrow\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=-\\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{M}\\left(h_{n}-\\mu\\right)^{2}-\\frac{M}{2}\\ln\\left(2\\pi\\right)-M\\,\\ln\\left(\\sigma\\right).\n",
        "\\end{equation}\n",
        "\n",
        "Para obtener los valores de $\\mu$ y $\\sigma$ que maximicen al logaritmo de la función de verosimilitud, derivamos respecto a $\\mu$ y $\\sigma$ respectivamente:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{d}{d\\mu}\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=0\n",
        "\\\\\n",
        "\\Rightarrow\\mu=\\frac{1}{M}\\left(\\sum_{n=1}^{M}h_{n}\\right),\n",
        "\\end{equation}\n",
        "\n",
        "lo cual coinicide con la fórmula de la media muestral planteada anteriormente. Ahora, derivando respecto a la desviación estándar e igualando a cero:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{d}{d\\sigma}\\ln\\left(p\\left(\\vec{h}|\\mu,\\sigma\\right)\\right)=0\n",
        "\\\\\n",
        "\\Rightarrow\\frac{1}{M}\\sum_{n=1}^{M}\\left(h_{n}-\\mu\\right)^{2}=\\sigma^{2},\n",
        "\\end{equation}\n",
        "\n",
        "lo cual es distinto a lo planteado en la ecuación 1 (normalización con\n",
        "$\\frac{1}{M-1}$). Por lo tanto, se obtiene lo que se conoce como un estimador de la varianza\n",
        "*sesgado*, el cual se *sobre-ajusta* a los datos (confía mucho en los datos) y tiende a subestimar la varianza de la distribución. Por esta razón, si se tienen pocas observaciones se recomienda usar el estimador sin sesgo de la ecuación 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9YAmJl3qps-"
      },
      "source": [
        "## Función Gaussiana multivariable\n",
        "\n",
        "Muchas veces será necesario lidiar con funciones de densidad de probabilidad definidas en un espacio de $D$ dimensiones ($D$ variables aleatorias distintas). En tal caso, cada observación corresponde a un vector $\\vec{x}=\\begin{bmatrix}x_{1}\\\\\\vdots\\\\x_{D}\\end{bmatrix}$ por lo que $\\vec{x}\\in\\mathbb{R}^{D}$. En particular, la distribución Gaussiana multivariable está dada por:\n",
        "\n",
        "\\begin{equation}\n",
        "f\\left(\\vec{x}|\\vec{\\mu},\\Sigma \\right)=\\mathcal{N}\\left(\\vec{x}|\\vec{\\mu},\\Sigma\\right)=\\frac{1}{\\left(2\\pi\\right)^{D/2}}\\frac{1}{\\textrm{det}\\left(\\Sigma\\right)^{1/2}}\\exp\\left(-\\frac{1}{2}\\left(\\vec{x}-\\vec{\\mu}\\right)^{T}\\Sigma^{-1}\\left(\\vec{x}-\\vec{\\mu}\\right)\\right),\n",
        "\\end{equation}\n",
        "\n",
        "donde:\n",
        "* El determinante $\\textrm{det}\\left(\\Sigma\\right)$ corresponde al volumen del \"cuerpo\" definido por la matriz de covarianza, y es un escalar que forma parte del coeficiente de normalización.\n",
        "* El exponente $-\\frac{1}{2}\\left(\\vec{x}-\\vec{\\mu}\\right)^{T}\\Sigma^{-1}\\left(\\vec{x}-\\vec{\\mu}\\right)$ es un escalar, con el vector $\\vec{\\mu}$ definido como el valor medio en cada dimensión $\\vec{\\mu}=\\left(\\mu_{1},\\ldots\\mu_{D}\\right)$. Observe que si $\\Sigma$ es la matriz de identidad, al aplicar la raíz cuadrada al término $\\left(\\vec{x}-\\vec{\\mu}\\right)^{T}\\Sigma^{-1}\\left(\\vec{x}-\\vec{\\mu}\\right)$ obtendríamos  la distancia euclidiana entre $\\vec{x}$ y  $\\vec{\\mu}$.\n",
        "* La **matriz de covarianza** $\\Sigma$ debe ser positivamente definida, es decir, su forma cuadratica para cualquier vector $\\vec{x}$ no nulo es positiva: $\\overrightarrow{x}^{T}\\Sigma\\overrightarrow{x}>0$.\n",
        "\n",
        "Para entender mejor el significado de la función Gaussiana multivariable, considere un caso simple en que la función Gausiana está dada por $f\\left(\\vec{x}\\right)=\\mathcal{N}\\left(\\vec{x}|\\mu,\\Sigma\\right)$, con $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$, y una matriz de covarianza diagonal\n",
        "$\\Sigma$, por lo que entonces tenemos que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{ccc}\n",
        "\\vec{x}=\\begin{bmatrix}x_{1}\\\\\n",
        "x_{2}\n",
        "\\end{bmatrix} & \\vec{\\mu}=\\begin{bmatrix}\\mu_{1}\\\\\n",
        "\\mu_{2}\n",
        "\\end{bmatrix} & \\Sigma=\\begin{bmatrix}\\sigma_{1}^{2} & 0\\\\\n",
        "0 & \\sigma_{2}^{2}\n",
        "\\end{bmatrix}\\end{array},\n",
        "\\end{equation}\n",
        "\n",
        "ahora, según la expresión general multi-variable de la función Gaussiana, se tendría para el exponente en forma cuadrática:\n",
        "\n",
        "\\begin{equation}\n",
        "-\\frac{1}{2}\\left(\\vec{x}-\\vec{\\mu}\\right)^{T}\\Sigma^{-1}\\left(\\vec{x}-\\vec{\\mu}\\right)=-\\frac{1}{2}\\left(\\begin{bmatrix}x_{1}-\\mu_{1} & x_{2}-\\mu_{2}\\end{bmatrix}\\right)\\begin{bmatrix}\\sigma_{1}^{2} & 0\\\\\n",
        "0 & \\sigma_{2}^{2}\n",
        "\\end{bmatrix}^{-1}\\left(\\begin{bmatrix}x_{1}-\\mu_{1}\\\\\n",
        "x_{2}-\\mu_{2}\n",
        "\\end{bmatrix}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "Observe que para cualquier matriz diagonal $U=\\begin{bmatrix}\\lambda_{1} & 0 & 0\\\\\n",
        "0 & \\ddots & 0\\\\\n",
        "0 & 0 & \\lambda_{n}\n",
        "\\end{bmatrix}$, su inversa viene dada por:\n",
        "$U^{-1}=\\begin{bmatrix}\\frac{1}{\\lambda_{1}} & 0 & 0\\\\\n",
        "0 & \\ddots & 0\\\\\n",
        "0 & 0 & \\frac{1}{\\lambda_{n}}\n",
        "\\end{bmatrix}:$\n",
        "\n",
        "\\begin{equation}\n",
        "=-\\frac{1}{2}\\begin{bmatrix}\\frac{1}{\\sigma_{1}^{2}}\\left(x_{1}-\\mu_{1}\\right) & \\frac{1}{\\sigma_{2}^{2}}\\left(x_{2}-\\mu_{2}\\right)\\end{bmatrix}\\left(\\begin{bmatrix}x_{1}-\\mu_{1}\\\\\n",
        "x_{2}-\\mu_{2}\n",
        "\\end{bmatrix}\\right)=-\\frac{1}{2}\\left(\\frac{1}{\\sigma_{1}^{2}}\\left(x_{1}-\\mu_{1}\\right)^{2}+\\frac{1}{\\sigma_{2}^{2}}\\left(x_{2}-\\mu_{2}\\right)^{2}\\right).\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "Respecto al coeficiente de normalización se tiene que:\n",
        "\n",
        "\\begin{equation}\n",
        "\\textrm{det}\\left(\\Sigma\\right)^{1/2}=\\begin{vmatrix}\\sigma_{1}^{2} & 0\\\\\n",
        "0 & \\sigma_{2}^{2}\n",
        "\\end{vmatrix}^{1/2}=\\left(\\sigma_{1}^{2}\\sigma_{2}^{2}-0*0\\right)^{1/2}=\\sigma_{1}\\sigma_{2}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "por lo que entonces tal coeficiente de normalización en este caso viene dado por $\\frac{1}{{2\\pi}\\sigma_{1}\\sigma_{2}}$, con ello:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{1}{{2\\pi}\\sigma_{1}\\sigma_{2}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{1}{\\sigma_{1}^{2}}\\left(x_{1}-\\mu_{1}\\right)^{2}+\\frac{1}{\\sigma_{2}^{2}}\\left(x_{2}-\\mu_{2}\\right)^{2}\\right)\\right)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vvz3XN11f9z"
      },
      "source": [
        "# Conceptos básicos de estadística\n",
        "\n",
        "Dos de las tareas básicas que trataremos de resolver mediante el uso de estadística son:\n",
        "\n",
        "1. Identificar que distribución sigue una variable, por ejemplo, la distribución del consumo eléctrico de un hogar en KWs. Esto nos permitirá seleccionar técnicas de clasificación, selección de características, visualización de datos, etc.\n",
        "2. Comparar de forma estadísticamente significativa, el efecto de un *tratamiento* o un conjunto de tratamientos sobre una población con respecto a la misma población\n",
        "sin tratamiento. En el plano del aprendizaje automático y las ciencias\n",
        "de los datos, esto es útil para comparar distintos modelos (tratamientos).\n",
        "\n",
        "Las siguientes secciones sientan las bases para el posterior abordaje formal de ambos temas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ePhRJs1h8l"
      },
      "source": [
        "## Muestreo aleatorio\n",
        "\n",
        "En muchos problemas reales, no podemos acceder a todas las observaciones que un fenómeno aleatorio puede generar. Es por esto que se\n",
        "analiza una muestra o conjunto de observaciones, seleccionados de\n",
        "la población que estamos interesados en estudiar. Los parámetros\n",
        "reales de una población, muchas veces no es posible obtenerlos por\n",
        "esta limitación, por lo que se estiman usando\n",
        "una **muestra**, como lo ilustra la siguiente figura.\n",
        "\n",
        "![](image4.png)\n",
        "\n",
        "Observe que en estadística el concepto de muestra se refiere a un conjunto de observaciones. Una **población** consiste en la totalidad de las observaciones\n",
        "generadas a partir de un fenómeno, mientras que una **muestra** es un subconjunto de observaciones seleccionadas de una población.\n",
        "\n",
        "El modelado paramétrico utiliza una distribución de probabilidad para\n",
        "modelar una población. Por ejemplo, la población de las edades de\n",
        "los usuarios de una aplicación de e-commerce pueden presentar una\n",
        "distribución normal con media $\\mu$ y desviación estándar $\\sigma$.\n",
        "Para una **muestra de la población**, se representan sus dos\n",
        "momentos estadísticos como la media muestral $\\overline{x}$ y la desviación estándar muestral\n",
        "$s$. La siguiente figura muestra una\n",
        "ilustración del modelo de la población, y el modelo estimado a partir\n",
        "de la muestra, el cual se puede visualizar de forma no paramétrica\n",
        "con el histograma.\n",
        "\n",
        "![](image5.png)\n",
        "\n",
        "En muchas ocasiones es impráctico observar la población completa.\n",
        "La selección de muestras puede introducir un **sesgo** o *bias*, al seleccionarse de forma conveniente un conjunto de observaciones\n",
        "que favorezcan una hipótesis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwo2dLFMwJo2"
      },
      "source": [
        "### Ejemplo\n",
        "Supongamos que deseamos llegar a una conclusión sobre la proporción de personas en Costa Rica que prefieren\n",
        "una marca particular de automóvil. Sea $p$ el valor desconocido de\n",
        "esta proporción. No es práctico encuestar a cada individuo en la\n",
        "población para determinar el verdadero valor de $p$.\n",
        "\n",
        "Para hacer una inferencia sobre la verdadera proporción $p$, un procedimiento más razonable consiste en seleccionar una muestra aleatoria (de un tamaño\n",
        "apropiado) y estimar la proporción $\\overline{p}$ de personas en esta\n",
        "muestra que prefiere tal marca de automóvil. La proporción de la muestra $\\overline{p}$ se calcula dividiendo el número de individuos en la muestra\n",
        "que prefieren la marca de automóviles por el tamaño total de la muestra\n",
        "$n$. Como $\\overline{p}$ es una función de los valores observados en la\n",
        "muestra aleatoria y dado que muchas muestras aleatorias son posibles\n",
        "de una población, el valor de $\\overline{p}$ variará de una muestra a\n",
        "otra. Por lo tanto, $\\overline{p}$ se modela como una variable aleatoria y se conoce como una estadística muestral o simplemente una estadística."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45iqXa_14K2"
      },
      "source": [
        "## Estadísticas descriptivas básicas\n",
        "\n",
        "**Media muestral**. Para una muestra de datos multidimensionales $h$, el vector de medias $\\overrightarrow{\\mu}$, se define como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\overrightarrow{\\mu}=\\frac{1}{N}\\sum_{i=1}^{N}\\vec{h\\left[i\\right]}.\n",
        "\\end{equation}\n",
        "\n",
        "y la **covarianza muestral** $\\Sigma$ se estima como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma=\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(\\vec{h[i]}-\\vec{\\mu}\\right)\\left(\\vec{h[i]}-\\vec{\\mu}\\right)^{T}\n",
        "\\end{equation}\n",
        "\n",
        "Otra estadística descriptiva es la **moda** $\\overrightarrow{m}$, que\n",
        "se define como el valor de mayor ocurrencia, en un conjunto de datos $X$.\n",
        "\n",
        "Por ejemplo, para los siguientes datos multivariable:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{array}{c}\n",
        "\\overrightarrow{x}_{1}=\\left[1\\;2\\right]^{T}\\\\\n",
        "\\overrightarrow{x}_{2}=\\left[1\\;2\\right]^{T}\\\\\n",
        "\\overrightarrow{x}_{3}=\\left[3\\;2\\right]^{T}\\\\\n",
        "\\overrightarrow{x}_{4}=\\left[5\\;3\\right]^{T}\\\\\n",
        "\\overrightarrow{x}_{5}=\\left[8\\;7\\right]^{T}\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "La moda está dada por $\\overrightarrow{m}=\\left[1\\;2\\right]^{T}$,\n",
        "puesto que para cada dimensión, tales son los valores más frecuentes. En caso de empates entre los valores más comunes, todos estos se consideran la moda y entonces podríamos considerar a  $\\overrightarrow{m}$ como un vector de listas más bien.\n",
        "\n",
        "La **mediana $\\overrightarrow{\\nu}$** es una estadística que requiere de un ordenamiento de los datos,\n",
        "para posteriormente escoger el valor en la posición central de la secuencia ordenada. En el ejemplo anterior, la mediana viene dada por: $\\overrightarrow{\\nu}=\\left[3\\;2\\right]^{T}$. En el caso de arreglos con cantidad de elementos pares se suele calcular la mediana como la media de los dos valores centrales.\n",
        "\n",
        "Tanto la media, como la mediana, y la moda, miden la **tendencia\n",
        "central de los datos**. La mediana no toma en cuenta los valores extremos ni atípicos, lo cual la hace más robusta que la media ante la presencia de outliers (en caso de que no se deseen tomar en cuenta). Por otra parte, la media puede producir valores reales y no plausibles en variables discretas.\n",
        "\n",
        "La moda da una mejor aproximación del valor más probable, para muestras con funciones de densidad con inclinación, como lo muestra la siguiente figura, con inclinación positiva y negativa.\n",
        "\n",
        "![](image6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ_7lVN53TjX"
      },
      "source": [
        "## Visualización de muestras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exKUcXBfqs2Y"
      },
      "source": [
        "### Diagramas de troncos-hojas\n",
        "\n",
        "Para visualizar las muestras, además de los histogramas, comentados\n",
        "anteriormente, se utilizan los diagramas de *troncos-hojas*, donde\n",
        "se elige un subconjunto de digitos más significativos por observación,\n",
        "y se agrupan el resto de digitos menos significativos a la derecha. La siguiente figura muestra un ejemplo de un diagrama de *troncos-hojas*. Observe que la moda se puede visualizar más fácilmente; en este caso es 158. Por otra parte, la fila que contiene a la mediana (160), indica en la columna izquierda la cantidad de observaciones de la fila. Este diagrama es útil en set de datos pequeños.\n",
        "\n",
        "![](image7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osUPJxr3qxN-"
      },
      "source": [
        "### Cuartiles y deciles\n",
        "\n",
        "Al ordenar un conjunto de datos $h$ y dividirlo en cuatro partes\n",
        "iguales, los puntos de división se llaman cuartiles. El primer cuartil\n",
        "o el cuartil inferior, $q_{1}$, es un valor que es mayor que aproximadamente\n",
        "el 25\\% de las observaciones. El segundo cuartil, $q_{2}$,\n",
        "tiene aproximadamente el 50\\% de las observaciones por debajo de\n",
        "su valor. El segundo cuartil corresponde a la mediana. El\n",
        "tercer cuartil o superior, $q_{3}$, tiene aproximadamente el 75\\%\n",
        "de las observaciones por debajo de su valor. Un diagrama con deciles,\n",
        "divide los datos en 10 partes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66UGPTpTrhKU"
      },
      "source": [
        "### Histogramas con cubetas\n",
        "\n",
        "Un histograma $p\\left(x\\right)$ puede asociar cada barra o cubeta a una unidad en el intervalo de observaciones o a un subintervalo de longitud determinada,\n",
        "como lo muestra la siguiente figura. Esta última\n",
        "opción permite disminuir la posible *ralidad*, o bien cantidad de huecos del histograma,\n",
        "puesto que algunos valores pueden no suceder en una muestra. Para eliminar la ralidad del todo, es posible utilizar cubetas de ancho variable. Los histogramas son más estables y recomendados para conjuntos\n",
        "de datos con más de 100 muestras.\n",
        "\n",
        "![](image8.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDAgipSMshvY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb6kEO89r_Kn"
      },
      "source": [
        "### Diagramas de caja o *box plot*\n",
        "\n",
        "Los diagramas de caja representan múltiples atributos de los datos al mismo tiempo:\n",
        "\n",
        "- Mediana\n",
        "- Variación (anchura de la caja y los bigotes)\n",
        "- Simetría (o ausencia de ella)\n",
        "- Rango intercuartil (IQR) definido como la distancia entre el primer y el tercer cuartil\n",
        "- Valores extremos (graficados como puntos separados si se encuentran a más de 1.5 IQR de la caja)\n",
        "- Cuartiles 1, 2 y 3\n",
        "- Mínimo y máximo\n",
        "\n",
        "La siguiente figura muestra un diagrama de cajas, con valores extremos representados por puntos.\n",
        "\n",
        "![](image9.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Ox8Ydh2JAa"
      },
      "source": [
        "### Diagramas de series de tiempo\n",
        "\n",
        "Estos diagramas se utilizan para analizar la evolución de los datos a lo largo del tiempo. El eje horizontal normalmente representa el tiempo y el eje vertical representa el valor de la variable bajo estudio.\n",
        "\n",
        "Al ser graficados de esta forma, los datos pueden revelar tendencias, ciclos o, en general, patrones que se repiten a lo largo del tiempo.\n",
        "\n",
        "![](image10.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
